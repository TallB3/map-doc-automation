take a look at the pdfs about reflection and RAG in the knowledge folder. we need to learn from it in order to improve our accuracy on timestamp reference and quoting. 

when i look at map-doc outputs, i can see several issues arising consistently regarding content generated by our accuracy-service, meaning the high accuracy model. when it does chapter timestamps, or reccomends shorts, or comes up with quotes - it's main issue is that the timestamps that he returns are usually wildly off. the good news is that i dont think he hallucinates a lot - meaning, i didnt see many cases where he quotes thing that werent said at all, or said but very much differently than how he describes it. when he says that something was said, it's usually the case that it was indeed said (albeit, the timestamp itself is wrong), but there is still room for improvement - sometimes the quote itself is not accurate enough, its a bit of a paraphase. its comlicated though, because sometimes it is actually preferable that the quote is slightly altered. for example, if the quote is "and then, ahh, we thought that like, it's best that we do it asap" its probably better that gemini quotes it as "and then we thought that it's best that we do it asap". 

i guess the way to solve this has to do with reflection and RAG thechniques mentioned in the knowledge folder. for RAG, i guess that points of reference for timestamps for example can be derived from the transcript.txt file given to the model in the prompt, but its worth to consider also using the raw-transcript.json file for even more accrate - as every token has its exact timestamp there! we simply dont give it to the model in the prompt to begin with because its so much bigger than the transcipt.txt file, but i believe that it can defintely be used specifically when we want to verify timestamps for example.

i need you to improve the accuracy of our generated content tremendously. i trust you to set your own benchmarks for accuracy, and also be responsible for assessing the accuracy of a gemini output. meaning, if inside the map-doc its written that X was said in Y, its up to you claude to go and verify by looking at the transcript(s) that X was actually said, and that it was indeed said in Y. Alternitevely, you can consider to also write tests (perhaps using the TDD agent) that asses the accuracy of generations.

I will be satisfied when the generated map-docs are very accurate.

to improve your efficiency, start your workflow by websearching and fetching any info that could expand your skills even beyond the pdfs i already provided you. after digesting all of the information, i'd like you to create an .md file concetrating all of your relevant findings, in case this enhacncement will be implemented over many instances of claude code.